# 2023-05-21

## MASK

mask操作一般分为两种，第一种pad-mask，主要处理batch中不定长的文本，第二种attention-mask,主要在transformer的decoder模块中，防止下一个字的信息泄露。

### 1，embedding-mask

在NLP中，一个常见的问题是输入序列长度不等，而mask可以帮助我们处理。

虽然RNN等模型可以处理不定长的input，但是在实践中，需要对input做batchize，转换成固定大小的tensor，方便矩阵操作，

常见的shape如下：(seq_len, batch_size, dim)。 

举个例子：
`case 1: I like cats.`
`case 2: He does not like cats.` 

假设默认的seq_len是5，一般会对case 1做pad处理，变成 

```
I like cats <PAD> <PAD>
```

在上述例子数字编码后，开始做embedding，而pad也会有embedding向量，但pad本身没有实际意义，参与训练可能还是有害的。因此，有必要维护一个mask tensor来记录哪些是真实的value，

上述例子的两个mask如下： 

```
1 1 1 0 0`
`1 1 1 1 1`
后续再梯度传播中，mask起到了过滤的作用，在pytorch中，有参数可以设置：
nn.Embedding(vocab_size, embed_dim,padding_idx=0)
```

代码

```python
import torch
from torch import nn
from torch import LongTensor

vocab_size = 100
embed_dim = 5

input = LongTensor([1,2,3,0,0,0])
print(input.shape)

embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0)
output = embedding(input)
print(output)
print(output.shape)
```

输出：

```python
torch.Size([6])
tensor([[-1.0128e+00, -4.9203e-01,  2.5148e-01, -5.7793e-01, -8.5760e-01],
        [ 4.6517e-01, -3.7407e-02, -1.2816e-03, -5.5597e-01,  2.8934e-01],
        [-4.1323e-01, -2.5838e-01,  1.5559e+00,  7.0030e-01, -1.9402e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],
       grad_fn=<EmbeddingBackward0>)
torch.Size([6, 5])
```

指定了embedding中pad的下标，embedding就会自动将pad的tensor换成全0，使得梯度下降和反向传播都不会在pad字上起作用，避免pad影响原句。

### 2，Bert中attention-mask探究



```python
import torch
from torch import nn
from torch import LongTensor
from transformers import AutoTokenizer, AutoModel

text = ["我爱祖国", "世界人民大团结"]

tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
bert = AutoModel.from_pretrained("bert-base-chinese")

input = tokenizer.batch_encode_plus(text,
                                    max_length=10,
                                    truncation=True,
                                    padding="max_length",
                                    return_tensors="pt")
print(input)

last_hidden_state, pooler_output = bert(input['input_ids'], input['token_type_ids'], input['attention_mask'], return_dict=False)

print(last_hidden_state.shape)

print(last_hidden_state[0][-1][:10])  # 第一句话最后一个字的向量中的前10个数字
print(last_hidden_state[0][-2][:10])  # 第一句话最后一个字的向量中的前10个数字
```



```python
{'input_ids': tensor([[ 101, 2769, 4263, 4862, 1744,  102,    0,    0,    0,    0],[ 101,  686, 4518,  782, 3696, 1920, 1730, 5310,  102,    0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}

torch.Size([2, 10, 768])

tensor([-0.3496,  0.1077, -0.8193,  0.0791,  0.8974,  0.2026,  0.5410,  0.0591, -0.9471, -0.5172], grad_fn=<SliceBackward0>)
tensor([-0.3441,  0.1093, -0.8319,  0.0947,  0.9009,  0.2181,  0.5442,  0.0543, -0.9391, -0.5559], grad_fn=<SliceBackward0>)
```

可以看到，和Embedding不同，即使指定了pad，Bert在pad位置输出的结果也不为0，值得怀疑Bert在推理过程中pad会不会影响原句。



# 2023-05-31

## Dropout

```python
# Drouout
input = torch.tensor(np.random.randn(10))
print(input.shape)
print(input)

dropout = nn.Dropout(0.2)

output = dropout(input)
print(output.shape)
print(output)
```

结果：

```python
torch.Size([10])
tensor([ 1.0846, -1.7892, -1.3069,  1.0584,  2.0671, -0.1879,  0.0652, -0.1369, -0.0686, -1.6679], dtype=torch.float64)
torch.Size([10])
tensor([ 1.3557, -2.2365, -0.0000,  0.0000,  2.5839, -0.2349,  0.0815, -0.1711, -0.0857, -2.0849], dtype=torch.float64)
```

无论input的形状如何，随机失活其中的20%，在多次训练过程中每次训练失活的地方不一样，可以直观理解成每次训练的是不同的模型，类似集成学习。

但是在推测过程中，只要指定了model.eval() ，模型中的dropout就不会运行,使得推理过程中不会损失语义信息。

# 2023-06-07

## bert扩充词表

```python
from transformers import BertModel, BertTokenizer

model_name = "./pretrained_model/bert-base-chinese"

tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

print(model.get_input_embeddings())#Embedding(21128, 768, padding_idx=0)

#未扩充词表的时候，编码后的结果
text = "[prompt_1][prompt_2]"
print(tokenizer.encode(text))#[101, 138, 8376, 9085, 8165, 142, 122, 140, 138, 8376, 9085, 8165, 142, 123, 140, 102]

#扩充词表
tokenizer.add_special_tokens({"additional_special_tokens": ["[prompt_1]", "[prompt_2]"]})

#扩充词表后的时候，编码后的结果
print(tokenizer.encode(text))#[101, 21128, 21129, 102]


#扩充词表后，模型的embedding也要对应扩充，否则出错
model.resize_token_embeddings(len(tokenizer))

print(model.get_input_embeddings())#Embedding(21130, 768)
```



```python
Embedding(21128, 768, padding_idx=0)
[101, 138, 8376, 9085, 8165, 142, 122, 140, 138, 8376, 9085, 8165, 142, 123, 140, 102]
[101, 21128, 21129, 102]
Embedding(21130, 768)
```

# 2023-06-08

## BN和LN的区别

Batch Normalization 的处理对象是对一批样本， Layer Normalization 的处理对象是单个样本。

Batch Normalization 是对这批样本的同一维度特征做[归一化](https://www.zhihu.com/search?q=归一化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"113233908"})， Layer Normalization 是对这单个样本的所有维度特征做归一化。

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-0ac1060e38f8ce8914d6a600bd63f854_b.jpg)

总结一下：
 BN、LN可以看作横向和纵向的区别。
经过归一化再输入[激活函数](https://www.zhihu.com/search?q=激活函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"113233908"})，得到的值大部分会落入[非线性函数](https://www.zhihu.com/search?q=非线性函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"113233908"})的线性区，导数远离导数饱和区，避免了梯度消失，这样来加速训练收敛过程。

BatchNorm这类[归一化技术](https://www.zhihu.com/search?q=归一化技术&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"113233908"})，**目的就是让每一层的分布稳定下来**，让后面的层可以在前面层的基础上安心学习知识。

BatchNorm就是通过对[batch size](https://www.zhihu.com/search?q=batch size&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"113233908"})这个维度归一化来让分布稳定下来。LayerNorm则是通过对Hidden size这个维度归一。



BN公式：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-46d5b41128928187f2d288191a3117d0_b.jpg)

# 2023-06-12

## python多进程-生产者和消费者模型



```python
from multiprocessing import Process, Queue
import time
import random
def producer(queue):
    while True:
        data = generate_data()  # 替换成实际的数据生成函数
        queue.put(data)
        time.sleep(1)  # 每隔一秒产生一次数据

def consumer(queue):
    while True:
        data = queue.get()
        process_data(data)  # 替换成实际的数据处理函数

def generate_data():
    # 这里可以生成数据的逻辑
    return random.random()

def process_data(data):
    # 这里可以处理数据的逻辑
    print("Processing data:", data)

if __name__ == '__main__':
    queue = Queue()
    p1 = Process(target=producer, args=(queue,))
    p2 = Process(target=consumer, args=(queue,))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
```

两个进程共享一个队列，一个不断生产，一个不断消费

本意是想探究roberta预训练过程中数据是怎样[不断动态掩码](https://github.com/Tongjilibo/bert4torch/tree/master/examples/pretrain/roberta_pretrain)的，是否能使用多进程的形式，一个文件不断生成，一个文件不断读取，结果发现两个代码中的queue无法共享，发现bert4torch中的与训练做法是一个代码不断生成并将语句存储到文件系统中，一个代码不断从文件系统中读取，从而实现真正的动态掩码。

不过与此同时也熟悉了多进程的生产者消费者模型，收获颇多。



## DataLoader中的num_workers

num_workers可以指定额外的进程对数据进行预先处理，保证数据供给能够赶得上模型训练的速度。

num_workers=0表示只有主进程去加载batch数据，这个可能会是一个瓶颈。

num_workers = 1表示只有一个worker进程用来加载batch数据，而主进程是不参与数据加载的。这样速度也会很慢。

num_workers>0 表示只有指定数量的worker进程去加载数据，主进程不参与。增加num_works也同时会增加cpu内存的消耗。所以num_workers的值依赖于 batch size和机器性能。

一般开始是将num_workers设置为等于计算机上的CPU数量

最好的办法是缓慢增加num_workers，直到训练速度不再提高，就停止增加num_workers的值。



# 2023-06-13

## DottableDict使用



平时访问字典使用类似于：dict['name']的方式，如果能通过dict.name的方式访问会更方便，DottableDict就可以提供这种操作。

```python
class DottableDict(dict):
    def __getattr__(self, item):
        value = self.get(item)
        if isinstance(value, dict):
            return DottableDict(value)
        return value

    def __setattr__(self, key, value):
        self[key] = value

    def __delattr__(self, item):
        del self[item]

    def print(self, indent=0):
        for key, value in self.items():
            if isinstance(value, DottableDict):
                print(' ' * indent + f'{key}:')
                value.print_properties(indent + 2)
            elif(isinstance(value, dict)):
                print(' ' * indent + f'{key}:')
                DottableDict(value).print_properties(indent + 2)
            else:
                print(' ' * indent + f'{key}: {value}')
```



上述代码可以将dict转换成DottableDict类，将属性的访问方式以.进行，同时可以以yaml的形式打印属性值。

# 2023-06-26

## 深度学习中的参数量

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-7fc916c02a94feb1fda71310c9c4e32c_b.jpg)

大规模预训练模型参数两巨大，其中的参数个数通常较多，一般采用M和B计数。

M代表百万（1e6）

B代表十亿（1e9）

## GPT1,2,3的异同

### GPT1

#### 背景

2018 年，那个时候 NLP 在深度学习上基本还处于 word2vec 以及为不同任务做定制化深度模型的情况，虽然已经有 ELMo 这类预训练模型出现，但是其影响力还远远不足。在这个背景下，GPT 第一代预训练语言模型出现了。

GPT 原文标题为 *Improving Language Understanding by Generative Pre-Training*，即使用通用的预训练模型来提升语言理解能力（Generative Pre-Training 也可理解为“生成式预训练”）。GPT 这个名字就来源于 Generative Pre-Training。

从论文标题可以引出了**两个问题**：

1. 什么是通用？在学习通用的，迁移性强的文本特征表达时，什么目标函数是有效的？
2. 有了通用的特征表达之后，如何将它迁移到不同下游任务？

**GPT 使用了预训练 + 微调的方式解决了这两个问题。**

#### 模型结构

在预训练阶段，GPT 选择 transformer 的 decoder 部分作为模型的主要模块，transformer 是 2017年 google 提出的一种[特征抽取模型](https://www.zhihu.com/search?q=特征抽取模型&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"609716668"})，GPT 以多层 transformer 堆叠的方式构成了整个预训练模型结构。

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-50c97a230b651da88d5209cb5faabb3c_b.jpg)

假设有一段文本，把每个词计作 $u_i$ ，GPT 使用标准的语言模型目标函数来最大化下面的似然函数：
$$
L1(U)=∑ilogP(ui|ui−k,...,ui−1;Θ)
$$
具体来说是要预测每个词 $u_i$ 的概率，这个概率是基于它前面$u_{i-k}$到$u_{i−1}$个词，以及模型 $\Theta$。这里的 k 表示上文的窗口大小，理论上来讲 k 取的越大，模型所能获取的上文信息越充足，模型的能力越强。

模型对输入 U 进行特征嵌入得到 transformer 第一层的输入$h_0$，再经过多层 transformer 特征编码，使用最后一层的输出即可得到当前预测的概率分布：


$$
h_0 = UW_e + W_p
$$

$$
h_l = transformer\_block(h_{l-1})
$$

$$
P(u) = softmax(h_nW_e^T)
$$

其中， $W_e$ 为词嵌入矩阵，$W_p $为位置嵌入矩阵，$h_l $为第$l$ 层 transformer 的输出，$h_n $为最后一层 transformer 的输出，$n $为模型层数。

在微调阶段，在有特定下游任务标签的情况下，给定输入序列 $x_1$ 到 $x_m$ ，预测 $y $的概率，即将序列输入到预训练好的模型中，得到最后一层 transformer 的最后一个 token $x^m$ 的特征 $h_l^m$ ，再经过预测层就可以得到对应标签的概率分布：


$$
P(y|x^1,...,x^m) = softmax(h_l^mW_y)
$$
微调阶段的目标函数为：


$$
L_2(U) = \sum^{}_{(x,y)}{logP(y|x^1,...,x^m)}
$$


最后将两个目标函数联合训练得到的效果最好，即最终目标函数为：
$$
L_3(U) = L_2(U) + \lambda * L_1(U)
$$
以现在的视角来看，Transformer 具有更加结构化的记忆单元来解决长距离依赖问题，处理更长的文本信息，从而使得学习到的特征在各个任务中的迁移具有更强的鲁棒性。

我们都知道，Transformer 模型一开始是用来做 seq2seq 任务的，所以它包含编码器和解码器两个部分；他们两者的区别主要是，编码器在抽取序列中某一个词的特征时能够看到整个序列中所有的信息，即上文和下文同时看到；而解码器中因为有mask机制的存在，使得它在编码某一个词的特征时只能看到自身和它之前的文本信息。GPT 模型选择了Transformer 的 decoder，也就是解码器的部分，也正是因为 GPT 的预训练目标函数选取的是标准的语言模型目标函数，使得模型在预测某一个词的时候只考虑上文信息而不参考下文。

大家知道，BERT 在预训练的时候选择的不是标准的语言模型作为目标函数，而是一种 mask 语言模型，也就是在预测句子中某一个词的时候可以同时看到它前后的所有上下文信息，类似于一种完形填空任务，所以 BERT 选择的是 Transformer 的编码器模块。

编码器和解码器的选取倒不是 GPT 和 BERT 的区别，它们的区别主要是预训练目标函数的选取，有人认为 GPT 选择的是一个更难的训练目标，它是根据前面的信息去预测下文，预测未来肯定是比完形填空难度要更大的。这也能从某种程度上解释了为什么相同规模的 GPT 和 BERT 模型，GPT 的效果要比 BERT 差。但是从另一个角度去想，如果能够把预测未来这个事情做好的话，它最终所能达到的效果的天花板一定是更高的，这可能也是 OpenAI 从一开始到现在一直坚持使用标准语言模型目标函数来做预训练模型的其中一个原因吧，当然这只是一种猜想。事实证明，从 GPT-3 开始，到最近的 ChatGPT，OpenAI 所取得的令人惊艳的效果也一定程度上证明了他们的选择的正确性。

#### 模型训练

训练数据方面，初代 GPT 使用了 BooksCorpus 数据集，文本大小约 5 GB，包含 7400w+ 的句子。该数据集是由约 7000 本独立的、不同风格类型的书籍组成。选择该数据集主要的好处是书籍文本包含大量高质量长句，保证模型学习长距离信息依赖。

模型的一些关键参数为：

| 参数                | 取值    |
| ------------------- | ------- |
| transformer 层数    | 12      |
| 特征维度            | 768     |
| transformer head 数 | 12      |
| 总参数量            | 1.17 亿 |

#### 下游任务微调

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-07a07e5db10878c34accb8d28d70974c_b.jpg)

如上图所示，分别例举了 NLP 中四个常见任务（文本分类、文本蕴含、文本相似度、问答任务）作为下游任务应用到 GPT 模型时，其输入序列是如何构造的，以及对应的预测层是如何设计的。

总的来说，都是通过在序列前后添加 Start 和 Extract 特殊标识符来表示开始和结束，序列之间添加必要的 Delim 标识符来表示分隔，当然实际使用时不会直接用 “Start/Extract/Delim” 这几个词，而是使用某些特殊符号。基于不同下游任务构造的输入序列，使用预训练的 GPT 模型进行特征编码，然后使用序列最后一个 token 的特征向量进行预测。

可以看到，不论下游任务的输入序列怎么变，最后的预测层怎么变，中间的特征抽取模块都是不变的，具有很好的迁移能力。

#### 初代 GPT 总结

初代 GPT 到底做了什么？有哪些贡献？

**第一**，它是最早一批提出在 NLP 任务上使用 pre-train + fine-tuning 范式的工作。

**第二**，GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间，如下图：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-340623a614d770bfbc193be8a0395f82_b.jpg)

**第三**，预训练模型具有 [zero-shot](https://www.zhihu.com/search?q=zero-shot&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"609716668"}) 的能力，并且能随着预训练的进行不断增强，如下图：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-acab7c8cfa2ee3d2becec00ee3784d6b_b.jpg)

值得注意的是，上述第二和第三点，也直接预示着后续 GPT-2 和 GPT-3 的出现。

其实 pre-train + fine-tuning 在计算机视觉里面早在好多年前已经成为主流的算法，但是在 NLP 中一直没有流行起来，主要还是因为在 NLP 里面没有像 ImageNet 那样大规模标好的数据集，这也导致相当一段时间内，深度学习在 NLP 的进展相对比较缓慢，直到 GPT 和 BERT 的出现才渐渐打开局面。

如果说使用大规模无标注的文本进行模型的预训练使 NLP 的发展往前走了一大步，那么 GPT 系列一直在努力推动的 zero-shot 可以说是走了另一大步。

为了进一步验证 zero-shot 的能力，OpenAI 在 GPT-1 提出一年后，推出了 GPT-2。



### GPT-2

#### 背景

GPT-2 原文标题为 *Language Models are Unsupervised Multitask Learners*，字面意思为语言模型是一种无监督多任务学习器。

标题中的多任务学习与我们常规理解的有监督学习中的多任务不太一样，这里主要是指模型从大规模数据中学到的能力能够直接在多个任务之间进行迁移，而不需要额外提供特定任务的数据，因此引出了 GPT-2 的主要观点：**zero-shot**。

不论是 GPT-1 还是 BERT，NLP 任务中比较主流的 pre-train + fine-tuning 始终还是需要一定量的下游任务有监督数据去进行额外的训练，在模型层面也需要额外的模块去进行预测，仍然存在较多人工干预的成本。GPT-2 想彻底解决这个问题，通过 zero-shot，在迁移到其他任务上的时候不需要额外的标注数据，也不需要额外的模型训练。

在 GPT-1 中，下游任务需要对不同任务的输入序列进行改造，在序列中加入了开始符、分隔符和结束符之类的特殊标识符，但是在 zero-shot 前提下，我们无法根据不同的下游任务去添加这些标识符，因为不进行额外的微调训练，模型在预测的时候根本不认识这些特殊标记。所以在 zero-shot 的设定下，不同任务的输入序列应该与训练时见到的文本长得一样，也就是以自然语言的形式去作为输入，例如下面两个任务的输入序列是这样改造的：

> 机器翻译任务：translate to french, { english text }, { french text }
> 阅读理解任务：answer the question, { document }, { question }, { answer }

为什么上述输入序列的改造是有效的？或者说为什么 zero-shot 是有效的？这里引用原文的一句话：

> Our approach motivates building as large and diverse a dataset as possible in order to **collect natural language demonstrations of tasks** in as varied of domains and contexts as possible.

大概意思是，从一个尽可能大且多样化的数据集中一定能收集到不同领域不同任务相关的自然语言描述示例，例如下图中展示了英法互译任务在自然语言中出现的示例，表明了不同任务的任务描述在语料中真实存在：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-d1ac178652e8ee1e5f69bcf29b983e3f_b.jpg)

所以 GPT-2 的核心思想就是，**当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调**。

#### 模型结构

在模型结构方面，整个 GPT-2 的模型框架与 GPT-1 相同，只是做了几个地方的调整，这些调整更多的是被当作训练时的 trick，而不作为 GPT-2 的创新，具体为以下几点：

1. 后置层归一化（ post-norm）改为前置层归一化（ pre-norm ）;
2. 在模型最后一个自注意力层之后，额外增加一个层归一化;
3. 调整参数的初始化方式，按残差层个数进行缩放，缩放比例为 1 : $\sqrt[]{n}$ ;
4. 输入序列的最大长度从 512 扩充到 1024;

其中，关于 post-norm 和 pre-norm 可以参考《Learning Deep Transformer Models for Machine Translation》。两者的主要区别在于，post-norm 将 transformer 中每一个 block 的层归一化放在了残差层之后，而 pre-norm 将层归一化放在了每个 block 的输入位置，如下图所示：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-73b126d30e470335fe5bdf06235b629f_b.jpg)

GPT-2 进行上述模型调整的主要原因在于，随着模型层数不断增加，梯度消失和梯度爆炸的风险越来越大，这些调整能够**减少预训练过程中各层之间的方差变化，使梯度更加稳定**。

最终 GPT-2 提供了四种规模的模型：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-b381d2c0ac192ecea05790cc2e26a3fe_b.jpg)

其中 117M 参数等价于 GPT-1 模型，345M 参数模型用于对标同期的 BERT-large 模型。

#### 训练数据与实验效果

在训练数据方面，为了保证 zero-shot 的效果，必须要足够大且覆盖面广。所以 GPT-2 专门爬取了大量的网络文本数据，最后得到的数据集叫 WebText。它选取了 Reddit 上的高质量帖子，最终得到 4500w 网页链接，800w 有效的文本文档，语料大小为 40G。

在实验效果上，由于 GPT-2 主要是做 zero-shot，所以在实验部分，很多的实验对比都是在无监督的设定下进行的，也就是说他对比的都是无监督的算法。

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-3d538b8e74573aa648cd9f1350fcc820_b.jpg)

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-69453d96554845d7c2949e8d6ac24c72_b.jpg)

从上述效果可以看到，GPT-2 在较多任务上对比无监督算法取得了一定的提升，证明了 zero-shot 的能力。但是，在很多任务上与有监督微调的方法相比还是有一些差距的，这可能也是 GPT-2 在当时影响力没有那么大的一个原因。



#### 与 GPT-1 的区别

整体来看，GPT-2 相比于 GPT-1 有如下几点区别：

> 1. **主推 zero-shot**，而 GPT-1 为 pre-train + fine-tuning；
> 2. **训练数据规模更大**，GPT-2 为 800w 文档 40G，GPT-1 为 5GB；
> 3. **模型大小**，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数；
> 4. **模型结构调整**，层归一化和参数初始化方式；
> 5. **训练参数**，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等；

------

### GPT-3

#### 背景

虽然 GPT-2 主推的 zero-shot 在创新度上有比较高的水平，但是由于其在效果上表现平平，所以在业界并没有取得比较大的影响力，而 GPT-3 正是为了解决效果上的问题而提出的。GPT-3 不再去追求那种极致的不需要任何样本就可以表现很好的模型，而是考虑像人类的学习方式那样，仅仅使用**极少数样本**就可以掌握某一个任务，因此就引出了 GPT-3 标题 *Language Models are **Few-Shot** Learners*。

这里的 few-shot 不是像之前的方式那样，使用少量样本在下游任务上去做微调，因为在 GPT-3 那样的参数规模下，即使是参数微调的成本也是高到无法估计。

#### 模型结构

在模型结构上，GPT-3 延续使用 GPT 模型结构，但是引入了 Sparse Transformer 中的 sparse attention 模块（稀疏注意力）。

sparse attention 与传统 self-attention（称为 dense attention） 的区别在于：

> dense attention：每个 token 之间两两计算 attention，复杂度 O(n²)
> sparse attention：每个 token 只与其他 token 的一个子集计算 attention，复杂度 O(n*logn)

具体来说，sparse attention 除了相对距离不超过 k 以及相对距离为 k，2k，3k，... 的 token，其他所有 token 的注意力都设为 0，如下图所示：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-f6312101c0304b9b47171779a6ca3cc4_b.jpg)

使用 sparse attention 的好处主要有以下两点：

1. **减少注意力层的计算复杂度**，节约显存和耗时，从而能够处理更长的输入序列；
2. **具有“局部紧密相关和远程稀疏相关”的特性**，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少；

关于 sparse attention 详情可参考《Generating Long Sequences with Sparse Transformers》。

最终 GPT-3 在训练过程中得到了如下不同规模的模型：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-7fc916c02a94feb1fda71310c9c4e32c_b.jpg)

其中规模最大的模型称为 GPT-3，模型参数量为 1750 亿。

#### 下游任务评估方法

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-eb547df80052d58f2eefa9e857a50f0b_b.jpg)

如上图所示，GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：

> **Zero-shot**：仅使用当前任务的自然语言描述，不进行任何梯度更新；
> **One-shot**：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；
> **Few-shot**：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；

其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是：

1. 【**本质区别**】fine-tuning 基于标注数据对模型参数进行更新，而 in-context learning 使用标注数据时不做任何的梯度回传，模型参数不更新；
2. in-context learning 依赖的数据量（10～100）远远小于 fine-tuning 一般的数据量；

最终通过大量下游任务实验验证，Few-shot 效果最佳，One-shot 效果次之，Zero-shot 效果最差：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-62ed9f372ee56c4a81298b191bf56927_b.jpg)

上图中，横坐标为模型参数量，纵坐标为任务精度，图中大量灰色线表示不同下游任务，橙色/绿色/蓝色线是下游任务效果的平均值。

#### 训练数据

由于 GPT-3 在模型规模上的扩大，在训练数据方面也必须进行扩充来适配更大的模型使其发挥出相应的能力。

GPT-3 使用了多个数据集，其中最大的是 CommonCrawl，原始未处理的数据达到了 45TB，其实在 GPT-2 的时候他们就有考虑使用这个数据集，但是后来还是觉得这个数据集太脏了所以没用，但是现在 GPT-3 的模型规模太大了，使得训练对数据量的需求也增加了很多，他们不得不重新考虑这个数据集。因此，他们必须在这个数据集上做一些额外的数据清洗工作来尽量保证数据的质量。

数据处理主要包括以下几个部分：

1. 使用高质量数据作为正例，训练 LR 分类算法，对 CommonCrawl 的所有文档做初步过滤；
2. 利用公开的算法做文档去重，减少冗余数据；
3. 加入已知的高质量数据集；

其中“高质量数据”主要是指 BERT、GPT、GPT-2 使用过的数据，最终处理完成后使用的数据规模约 570G。

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-c99fdb08c8754fcd588cda3f41669462_b.jpg)

如上图所示，在实际实验过程中，对不同数据集按照一定的比例进行采样，这个比例不是按照原始数据量多少来划分的，不然这里基本采样到的就都是 common crawl 的数据了，可以看到这里 common crawl 的数据量比其他几个多很多。进行采样的原因主要考虑到，就算做了一些数据清洗还是觉得 common crawl 的数据质量不如其他几个。最终采样的时候，虽然 common crawl 的数据量是其他几个数据集的上百倍，但是实际占比是 60%，有 40% 的数据是能够保证质量的。

#### 实验分析 

GPT-3 花了大部分篇幅介绍了各种 NLP 任务上的实验结果和分析，大家如果对某个任务感兴趣的话可以自行阅读一下论文对应的章节，本文就不做详细介绍了。

下图是 GPT-3 的一个重要分析结果：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-863f0e156da5f8a82042654b785175ed_b.jpg)

图中横坐标为计算量，可以简单理解为模型规模或者数据量（不止如此），纵坐标为任务精度。可以看到，**当我们想要线性的提升一个任务的效果时，往往需要指数级的提升模型的规模和所需的数据量**。

#### GPT-3 的局限性

虽然 GPT-3 取得了非常亮眼的效果，但是出于严谨的学术态度，论文里还是客观的分析了自己的一些局限性：

1. **当生成文本长度较长时**，GPT-3 还是会出现各种问题，比如重复生成一段话，前后矛盾，逻辑衔接不好等等；
2. **模型和结构的局限性**，对于某一些任务，比如填空类型的文本任务，使用单向的自回归语言模型确实存在一定的局限性，这时候如果同时考虑上文和下文的话，效果很可能会更好一些；
3. 预训练语言模型的通病，在训练时，语料中所有的词都被同等看待，对于一些虚词或无意义的词同样需要花费很多计算量去学习，**无法区分学习重点**；
4. **样本有效性或者利用率过低**，训一个模型几乎要把整个互联网上的文本数据全都用起来，这与我们人类学习时所需要的成本存在非常大的差异，这方面也是未来人工智能研究的重点；
5. 有一个不太确定的点是，模型到底是在“**学习**”还是在“**记忆**”？我们当然希望它能够学习，但是在使用数据量如此大的情况下，很难去判断它到底是什么样的；
6. 众所周知，GPT-3 的训练和使用**成本都太大**了；
7. GPT-3 跟很多深度学习模型一样，都是**不可解释**的，没办法知道模型内部到底是如何作出一系列决策的；
8. 模型最终呈现的效果取决于训练数据，这会导致模型会出现各种各样的“**偏见**”；

#### 某些社会影响 —— 模型“偏见”

GPT-3 可能会被拿来做一些坏事，造成一定的社会影响。比如生成新闻稿，散布一些不实的消息，生成垃圾邮件，钓鱼邮件，论文造假之类的。

这里以“种族偏见”和“性别偏见”为例：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-a17ba3c63e5c1c2bb87ee7ad4bf50e92_b.jpg)

上图展示了模型的“种族偏见”倾向，当给模型输入“The {种族} woman was very”时，可以根据后续预测词的概率分布简单分析出 GPT-3 对不同种族的人具有一定的出词倾向性。图中纵坐标的 0 表示一种正常水平，大于 0 表示比较正面的反馈，小于 0 表示比较负面的反馈。可以看到亚洲人在模型的评价里算是比较高的，但是最下面的那条线是黑人，所以当一个模型对不同种族的人的差异有这么大的时候，还是比较可怕的。

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-21c26984967c6291a148123eaf7f701e_b.jpg)

上图展示了模型的“性别偏见”，当给模型输入 “he was very” 或 “she was very” 时，GPT-3 给出的词的概率分布具有一定的倾向性，可能会产生一些具有偏见的词，比如形容男人时经常出现“Lazy”。

虽然 GPT-3 存在上述问题，不过当这些社会问题被拿出来讨论时，也侧面反映了 GPT-3 的效果及其影响力。

#### 与 GPT-2 的区别

整体来看，GPT-3 相比于 GPT-2 有如下几点区别：

> 1. **效果上**，超出 GPT-2 非常多，能生成人类难以区分的新闻文章；
> 2. **主推 few-shot**，相比于 GPT-2 的 zero-shot，具有很强的创新性；
> 3. **模型结构**略微变化，采用 sparse attention 模块；
> 4. **海量训练语料** 45TB（清洗后 570GB），相比于 GPT-2 的 40GB；
> 5. **海量模型参数**，最大模型为 1750 亿，GPT-2 最大为 15 亿参数；



### 区别总结

|      | 训练数据量                                                   | 模型结构                                                     | 设计目标                          | 参数量  |
| :--- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------------------- | ------- |
| GPT1 | BooksCorpus，文本大小约 5 GB，来源于书籍。                   | 12层transformer 结构                                         | 预训练-微调                       | 1.17亿  |
| GPT2 | WebText，40G，来源于网页的高质量帖子。                       | 和GPT1相比，<br />后置层归一化（ post-norm）改为前置层归一化（ pre-norm ）；<br />在模型最后一个自注意力层之后，额外增加一个层归一化；<br />调整参数的初始化方式，按残差层个数进行缩放，缩放比例为 1 : $\sqrt[]{n}$ ;<br />输入序列的最大长度从 512 扩充到 1024; | 做zero-shot                       | 15 亿   |
| GPT3 | 多个数据集，CommonCrawl,WebText2 ,Books1, Books1, Wikipedia.其中CommonCrawl最大，原始未处理的数据达到了 45TB，最终按比例抽取数据集，CommonCrawl占总量的60% | 引入了 Sparse Transformer 中的 sparse attention 模块（稀疏注意力） | 做Few-Shot，不再忙不追求zero-shot | 1750 亿 |



## matplotlib中文显示问题

只要在代码开始手动指定中文就好：

```python
from matplotlib import rcParams
rcParams['font.family'] = 'SimHei'
```

## 怎样画一个分组柱状图

```python
import matplotlib.pyplot as plt
import numpy as np

from matplotlib import rcParams
rcParams['font.family'] = 'SimHei'

# 定义数据
data1 = [3, 6, 9, 4, 3]
data2 = [2, 4, 8, 5, 8]
data3 = [5, 1, 7, 6, 2]

# 定义颜色
colors = ['#A9A9A9', '#808080', '#696969']

#柱子宽度
bar_width = 0.25

# 计算每组柱状图的位置
r1 = np.arange(len(data1))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]

# 绘制柱状图
plt.bar(r1, data1, color=colors[0], width=bar_width, edgecolor='white', label='Precision')
plt.bar(r2, data2, color=colors[1], width=bar_width, edgecolor='white', label='Recall')
plt.bar(r3, data3, color=colors[2], width=bar_width, edgecolor='white', label='F1')

# 在柱子上方添加数字
for i in range(len(data1)):
    plt.text(x = r1[i] - 0.07, y = data1[i] + 0.2, s = data1[i], size = 10)
    plt.text(x = r2[i] - 0.07, y = data2[i] + 0.2, s = data2[i], size = 10)
    plt.text(x = r3[i] - 0.07, y = data3[i] + 0.2, s = data3[i], size = 10)

# 添加图例
plt.legend()

# 添加标签和标题
plt.xlabel('句子长度')
plt.ylabel('评估指标')
plt.title('CMeIE')

# 调整x轴标签的位置
plt.xticks([r + bar_width for r in range(len(data1))], ['0-50', '50-100', '100-150', '150-200', '200+'])

# 显示图形
plt.show()
```

效果如下：

![image-20230626202626904](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/image-20230626202626904.png)

# 2023-06-27

## tokenizer在NER上的问题和解决方法

```python
from transformers import BertTokenizerFast

text = "世界Hello人民大团结"
tokenizer = BertTokenizerFast.from_pretrained("bert-base-chinese")

tokens = tokenizer.encode_plus(text, return_offsets_mapping=True)

print(tokens.input_ids)
print(tokens.token_type_ids)
print(tokens.attention_mask)
print(tokens.offset_mapping)
```

```
#输出

[101, 686, 4518, 100, 782, 3696, 1920, 1730, 5310, 102]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[(0, 0), (0, 1), (1, 2), (2, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (0, 0)]
```

可见经过tokenizer编码之后，text中的单词Hello被看作一个字，之后如果对tokens进行分类，那么输出长度和文本长度则会不一致，传统bertTokenizer无法解决这个问题，解决办法如下：

​	1，将text先转换成列表，每个元素是一个字符，则Hello是5个字符，此时即可对应上。

​	2，使用bertTokenizer进行编码，offset_mapping即可显示每个token的对应位置，分类后再将原文和位置一一对应即可。



# 2023-06-30

## prompt是什么

[Prompt范式的缘起](https://zhuanlan.zhihu.com/p/396971490)

[Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning](https://zhuanlan.zhihu.com/p/400790006)

[P-Tuning v2](https://zhuanlan.zhihu.com/p/619410558)

## 怎样突破bert最大句长512的限制

1.将bert的位置编码人为修改成（1\*1024），前512维使用原始的 （1\*512）初始化，后512维随机初始化

2.将bert的位置编码认为修改成（1\*1024），前512维使用原始的 （1\*512）初始化，后512维依旧使用原始的（1\*512）进行初始化

```python
from transformers import BertModel,BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")

text = "新华通讯社，简称新华社，是中国国家通讯社和世界性通讯社。现任社长傅华、总编辑吕岩松。" \
       "新华社的前身是1931年11月7日在江西瑞金成立的红色中华通讯社（简称红中社），1937年1月在陕西延安改为现名。" \
       "新华社总部设在北京，在全国除台湾省以外的各省区市均设有分社，在台湾省派有驻点记者，在一些重点大中城市设有支社或记者站，" \
       "在中国人民解放军、中国人民武装警察部队设有分支机构，在境外设有182个分支机构。 新华社建立了覆盖全球的新闻信息采集网络，" \
       "形成了多语种、多媒体、多渠道、多层次、多功能的新闻发布体系，集通讯社供稿业务、报刊业务、电视业务、经济信息业务、" \
       "互联网和新媒体业务等为一体，每天24小时不间断用中文、英文、法文、俄文、西班牙文、阿拉伯文、葡萄牙文和日文8种文字，" \
       "向世界各类用户提供文字、图片、图表、音频、视频等各种新闻和信息产品。 新华网是新华社主办的中国重点新闻网站，" \
       "被称为“中国最有影响力网站”，每天24小时以7种文字、通过多媒体形式不间断地向全球发布新闻信息，全球网站综合排名稳定在190位以内。" \
       "开通31个地方频道，承办中国政府网、中国平安网、中国文明网、振兴东北网等大型政府网站，形成了中国最大的国家级网站集群。1931年11月7日，新华社前身红色中华通讯社（简称红中社）在江西瑞金成立，" \
       "是中国共产党领导下成立最早的新闻机构。1934年10月，红中社随中央红军长征。" \
       "1937年1月，为适应革命斗争形势的需要，根据中央的决定，红中社在延安更名为新华通讯社。 [7]" \
       "1940年12月30日，新华社创办了延安新华广播电台，即中央人民广播电台的前身。" \
       "1944年9月1日，新华社开办了对国外英语广播。" \
       "1937年至1945年抗日战争时期，新华社在华北、晋绥、晋察冀、山东、华中各抗日民主根据地相继成立分社。当时，由于敌人的分割封锁，新华社成为抗日民主根据地对外发布新闻的唯一渠道。" \
       "1945年至1950年解放战争时期，新华社事业迅速发展。" \
       "1946年5月，新华社总社改组机构，同时向各主要战场派出随军记者或记者团。之后又在中国人民解放军各野战部队陆续建立前线分社和野战军总分社，在各兵团和军建立分社和支社。"

tokens = tokenizer.encode_plus(text, return_tensors="pt")
print("原文句长为：",len(text))
print("token长度为：",len(tokens))

# 不处理就会超长报错
# seq_out, pool_out = bert(**tokens)
# # RuntimeError: The size of tensor a (852) must match the size of tensor b (512) at non-singleton dimension 1

#新建一个embedding 用以替换bert中原来的pos_embedding
new_position_embeddings = torch.nn.Embedding(1024, 768)
#前512值还应是原embeddings
#后512为了能work也用原embeddings
new_position_embeddings.weight.data[:512] = bert.embeddings.position_embeddings.weight.data
new_position_embeddings.weight.data[512:] =  bert.embeddings.position_embeddings.weight.data

# 替换原来的embeddings
bert.embeddings.position_embeddings = new_position_embeddings
bert.embeddings.register_buffer("position_ids", torch.arange(1024).expand((1, -1)))


#推理
output = bert(**tokens)
print(output[0].shape)

# torch.Size([1, 852, 768]) 成功
```

3.苏剑林，[层次分解位置编码，让BERT可以处理超长文本 - 科学空间|Scientific Spaces](https://link.zhihu.com/?target=https%3A//kexue.fm/archives/7947)  bert4torch中也有实现 

4.使用longformer

bert 不能编码超过 512 长序列原因：

- 绝对位置编码
- attention的时间复杂度是 O(N2)，增大长度会指数级增大计算量。（但是苏剑林在文章中提到，句长几千的时候增长并不明显，以现代的计算量来看近似于线性增长）

NEZHA 使用相对位置编码



3中的层次分解位置编码思路如下：

思路是根据旧的编码p构建新的编码q，类似于坐标系的转换，其中q用一组基向量表示：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-7a87e93e3509121432645a77186bfe8e_b.jpg)

同时添加约束条件：在长度小于 n 时，保持$$q_i == p_i$$，这样就得到基底的表达式：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-d0c91c58f1a5f6ae68b2c48628f4c6b2_b.jpg)

经过上述变换，bert输入长度可以达到 $$ n^2  $$ 。

```python
from transformers import BertModel,BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")

text = "新华通讯社，简称新华社，是中国国家通讯社和世界性通讯社。现任社长傅华、总编辑吕岩松。" \
       "新华社的前身是1931年11月7日在江西瑞金成立的红色中华通讯社（简称红中社），1937年1月在陕西延安改为现名。" \
       "新华社总部设在北京，在全国除台湾省以外的各省区市均设有分社，在台湾省派有驻点记者，在一些重点大中城市设有支社或记者站，" \
       "在中国人民解放军、中国人民武装警察部队设有分支机构，在境外设有182个分支机构。 新华社建立了覆盖全球的新闻信息采集网络，" \
       "形成了多语种、多媒体、多渠道、多层次、多功能的新闻发布体系，集通讯社供稿业务、报刊业务、电视业务、经济信息业务、" \
       "互联网和新媒体业务等为一体，每天24小时不间断用中文、英文、法文、俄文、西班牙文、阿拉伯文、葡萄牙文和日文8种文字，" \
       "向世界各类用户提供文字、图片、图表、音频、视频等各种新闻和信息产品。 新华网是新华社主办的中国重点新闻网站，" \
       "被称为“中国最有影响力网站”，每天24小时以7种文字、通过多媒体形式不间断地向全球发布新闻信息，全球网站综合排名稳定在190位以内。" \
       "开通31个地方频道，承办中国政府网、中国平安网、中国文明网、振兴东北网等大型政府网站，形成了中国最大的国家级网站集群。1931年11月7日，新华社前身红色中华通讯社（简称红中社）在江西瑞金成立，" \
       "是中国共产党领导下成立最早的新闻机构。1934年10月，红中社随中央红军长征。" \
       "1937年1月，为适应革命斗争形势的需要，根据中央的决定，红中社在延安更名为新华通讯社。 [7]" \
       "1940年12月30日，新华社创办了延安新华广播电台，即中央人民广播电台的前身。" \
       "1944年9月1日，新华社开办了对国外英语广播。" \
       "1937年至1945年抗日战争时期，新华社在华北、晋绥、晋察冀、山东、华中各抗日民主根据地相继成立分社。当时，由于敌人的分割封锁，新华社成为抗日民主根据地对外发布新闻的唯一渠道。" \
       "1945年至1950年解放战争时期，新华社事业迅速发展。" \
       "1946年5月，新华社总社改组机构，同时向各主要战场派出随军记者或记者团。之后又在中国人民解放军各野战部队陆续建立前线分社和野战军总分社，在各兵团和军建立分社和支社。"

tokens = tokenizer.encode_plus(text, return_tensors="pt")
print("原文句长为：",len(text))
print("token长度为：",len(tokens))

# 不处理就会超长报错
# seq_out, pool_out = bert(**tokens)
# # RuntimeError: The size of tensor a (852) must match the size of tensor b (512) at non-singleton dimension 1


max_position = 1024

embeddings = bert.embeddings.position_embeddings.weight.data

#层次分解位置编码
alpha = 0.4
embeddings = embeddings - alpha * embeddings[:1]
embeddings = embeddings / (1 - alpha)
position_index = torch.arange(max_position)[:, None]
embeddings_x = torch.take_along_dim(embeddings, torch.div(position_index, embeddings.size(0), rounding_mode='trunc'), dim=0)
embeddings_y = torch.take_along_dim(embeddings, position_index % embeddings.size(0), dim=0)
embeddings = alpha * embeddings_x + (1 - alpha) * embeddings_y

new_embeddings = torch.nn.Embedding(max_position, 768)
new_embeddings.weight.data = embeddings

# 替换原来的embeddings
bert.embeddings.position_embeddings = new_embeddings
bert.embeddings.register_buffer("position_ids", torch.arange(max_position).expand((1, -1)))


#推理
output = bert(**tokens)
print(output[0].shape)

# torch.Size([1, 852, 768]) 成功
```

## 位置编码

[位置编码- 科学空间](https://kexue.fm/search/位置编码/#位置编码)



# 2023-07-08

## 关系分类和联合关系抽取的区别

### 联合关系抽取：

【临床表现】 按病程发展及主要临床表现，可分为急性、慢性及晚期血吸虫病。 （一）急性血吸虫病 多见于夏秋季，以小儿及青壮年为多。"->

 ["血吸虫病", "疾病@病理分型@疾病", "急性血吸虫病"], 
 ["急性血吸虫病", "疾病@病理分型@疾病", "血吸虫病"], 
 ["急性血吸虫病", "疾病@多发群体@流行病学", "小儿及青壮年"]



### 关系分类：

【临床表现】 按病程发展及主要临床表现，可分为急性、慢性及晚期<e1>血吸虫病</e1>。（一）<e2>急性血吸虫病</e2> 多见于夏秋季，以小儿及青壮年为多。"->病理分型

【临床表现】 按病程发展及主要临床表现，可分为急性、慢性及晚期<e2>血吸虫病</e2>。（一）<e1>急性血吸虫病</e1> 多见于夏秋季，以小儿及青壮年为多。"->病理分型

【临床表现】 按病程发展及主要临床表现，可分为急性、慢性及晚期血吸虫病。（一）<e1>急性血吸虫病</e1> 多见于夏秋季，以<e2>小儿及青壮年</e2>为多。"->多发群体



# 2023-07-10

## 什么是正则化Regularization？

Regularization是一种减小方差的策略，最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。

误差 = 偏差+方差+噪声

**偏差**度量算法的预测和真实结果的偏离程度。

**方差**度量同样大小的训练集的变动导致的学习性能的变化，刻画了数据扰动带来的影响。

**噪声**表达了在当前任务上任何学习算法所能达到的期望泛化误差下界，刻画的是学习问题本身的难度。

![image-20230710123538189](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/image-20230710123538189.png)

方差在我理解就是训练集和验证集上的性能之差，在训练集上表现好，在验证集上表现稍差，这种现象被称为过拟合，而正则化就是为了环节过拟合现象的技术。



### L1正则化L2正则化



![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-6903cf94f66a667c8e76430508e442e6_b.png)

L2在直观理解上更加稳定，而torch中的weight_decay就是L2正则化，也叫权重衰减

L2正则化就是在损失函数的后面再加上一个正则化项：

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-65ad1bad5ae6f9272d24e3192d4bafe8_b.png)

$$ C_0$$就是原本的损失函数，w是模型权重，1/2n是为了使权重平均，同时求导时抵消导数中的2，$$λ$$是人为指定的权重衰减系数，在一定范围内$$λ$$越大越有利于减少过拟合，《In Leslie’s paper, it is recommended to choose the largest weight dacay》论文验证了应该从1e-4左右尝试较好，torch中可能默认为1e-2。

### L1、L2的适用场景

由于L1、L2的特点，因此它们也有各自不同的适用场景。

L1：使模型中尽可能多的参数值为0，因此适用于：**模型剪枝，模型压缩，特征选择**。*是一种从改变模型结构的角度（减少模型参数的数量）解决过拟合的方式。*

L2：使模型中的所有参数值尽可能小，使得模型尽量不依赖于某几个特殊的特征，而是使每个特征都得到尽量均衡的权重，因此适用于解决**普通的过拟合问题**，*即从参数分布（让分布尽可能的均匀）的角度解决过拟合的问题，这也是常用的解决过拟合的方式。*



## torch给模型不同部分不同的学习率

```python
train_model = Model(config).to(device)

# print(*[k for k, v  in train_model.named_parameters() if "bert" in k], sep="\n")
param_optimizer = list(train_model.named_parameters())#模型的所有参数
param_pre = [(n, p) for n, p in param_optimizer if 'bert' in n]#与bert相关的所有参数
param_downstream = [(n, p) for n, p in param_optimizer if 'bert' not in n]#与bert无关的所有参数
optimizer_grouped_parameters = [#设置不同的学习率
    # pretrain model param
    {'params': [p for n, p in param_pre], 'lr': config.params.bert_lr},
    # downstream model
    {'params': [p for n, p in param_downstream], 'lr': config.params.down_stream_lr}
]

optimizer = optim.Adam(optimizer_grouped_parameters, config.params.bert_lr)
```



# 2023-07-14

## 高斯分布（正态分布）

**正态分布**（Normal Distribution），也称**常态分布**，又名**高斯分布**（Gaussian Distribution），是一个常见的连续概率分布。

若随机变量$$X$$服从一个数学期望为$$μ$$、方差为$$σ^2$$的正态分布，则记为$$X\backsim N(σ^2)$$。其概率密度函数为正态分布，期望值$$μ$$决定了其位置，其标准差$$σ^2$$决定了分布的幅度。正态分布的概率密度函数曲线呈钟形，因此又称之为钟形曲线（类似于寺庙里的大钟）。当$$μ$$=0，标准差$$σ^2$$=1时的正态分布是标准正态分布。

![img](https://typora-1259320645.cos.ap-beijing.myqcloud.com/typora/v2-f1b8fea14b509d1648afa76b162fc018_720w.webp)

# 2023-07-15

## 使用ltp对文本进行依赖树解析

```python
#pip install ltp

import torch
from ltp import LTP

device = "cuda" if torch.cuda.is_available() else "cpu"

text = "他叫汤姆(TOM)去拿外衣。"

ltp = LTP("LTP/base2").to(device)

output = ltp.pipeline(
    [text],
    tasks=[
        "cws",
        "dep",
    ])

cws = output.cws[0]
dep_head = output.dep[0]["head"]
dep_label = output.dep[0]["label"]

print(cws)
print(dep_head)
print(dep_label)

#['他', '叫', '汤姆', '(', 'TOM', ')', '去', '拿', '外衣', '。']
#[2, 0, 2, 5, 3, 5, 8, 2, 8, 2]
#['SBV', 'HED', 'DBL', 'WP', 'COO', 'WP', 'ADV', 'VOB', 'VOB', 'WP']
```

# 2023-07-25

## 使用unicodedata库去掉字符串中的全角字符

这个库里有一个 `normalize` 函数，可以将其他特殊的空格转换为标准的空格，然后使用replace替换就行

```python
import unicodedata as ucd

ucd.normalize('NFKC', msg).replace(' ', '') 
```
